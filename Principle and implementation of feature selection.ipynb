{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle and implementation of feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data and features determine the upper limit of machine learning, and models and algorithms only approximate this upper limit. Thus, feature engineering plays an important role in machine learning. In practical application, feature engineering is the key to the success of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In actual project, we may have a lot of features can be used, some characteristics of rich information, some characteristics of carrying information overlap, some features are irrelevant, if all features all as training without screening, frequently, dimension disaster problem, even will reduce the accuracy of the model. Therefore, we need to carry out feature screening to exclude invalid/redundant features and select useful features as training data of the model.\n",
    "\n",
    "1. Characteristics are classified according to importance\n",
    "\n",
    "Related features:\n",
    "- It is helpful for learning tasks (such as classification problems) and can improve the effect of learning algorithm.\n",
    "\n",
    "Irrelevant features:\n",
    "- There is no help for our algorithm, it will not bring any improvement to the effect of the algorithm;\n",
    "\n",
    "Redundancy features:\n",
    "- Does not bring new information to our algorithm, or information about this feature can be inferred from other features;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Purpose of feature selection\n",
    "It is unknown which feature is valid for a particular learning algorithm. Therefore, relevant features beneficial to the learning algorithm need to be selected from all features. In practice, dimension disaster often occurs. If only part of all features are selected to build the model, the running time of the learning algorithm can be greatly reduced and the interpretability of the model can be increased.\n",
    "\n",
    "3. Principle of feature selection\n",
    "The feature subset should be as small as possible, the classification accuracy should not be significantly reduced, the classification distribution should not be affected, and the feature subset should be stable and adaptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of feature selection\n",
    "### 1.Filter method (Filter type)\n",
    "\n",
    "Feature selection is performed first, and then the learner is trained, so the process of feature selection is independent of the learner. It is equivalent to filtering features first and then training classifiers with feature subsets.\n",
    "\n",
    "Main idea: \"score\" each one-dimensional feature, that is, assign weight to each one-dimensional feature, such weight represents the importance of the feature, and then rank according to the weight.\n",
    "\n",
    "Main methods:\n",
    "\n",
    "- Chi-squared test\n",
    "- Information gain\n",
    "- Correlation coefficient (s)\n",
    "\n",
    "Advantages: fast running speed, is a very popular feature selection method.\n",
    "\n",
    "Disadvantages: unable to provide feedback, the standard/specification of feature selection is completed in the feature search algorithm, the learning algorithm can not be transmitted to the feature search algorithm requirements. In addition, a feature may be treated as unimportant for any reason, but may become important in combination with other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Wrapper method\n",
    "The last classifier is directly used as the evaluation function of feature selection and the optimal feature subset is selected for a specific classifier.\n",
    "\n",
    "Main idea: consider the selection of subsets as a search optimization problem, generate different combinations, evaluate the combinations, and then compare them with other combinations. In this way, the selection of subsets is regarded as an optimization problem, which can be solved by many optimization algorithms, especially some heuristic optimization algorithms, such as GA, PSO (e.g. optimization algorithm - particle swarm algorithm), DE, ABC (e.g. optimization algorithm - artificial swarm algorithm), etc.\n",
    "\n",
    "Main method: recursive feature elimination algorithm.\n",
    "\n",
    "Advantages: feature search is carried out around the learning algorithm, the standard/specification of feature selection is carried out in the requirements of the learning algorithm, can consider the arbitrary learning bias of the learning algorithm, so as to determine the best sub-feature, really pay attention to the learning problem itself. Encapsulation can play a huge role because the learning algorithm has to be run every time a particular subset is tried, so it is possible to pay attention to the learning bias/inductive bias of the learning algorithm.\n",
    "\n",
    "Disadvantages: running speed is much slower than the filtering algorithm, the actual application of encapsulation method is not popular filtering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Embedded method (Embedded)\n",
    "If feature selection is embedded in model training, the training may be the same model, but after feature selection is completed, the features completed by feature selection and the superparameters trained by the model can be given to train and optimize again.\n",
    "\n",
    "Main idea: learn the best features to improve the accuracy of the model under the condition of the established model. That is, in the process of determining the model, the characteristics that are important for the training of the model are selected.\n",
    "\n",
    "Main methods: feature selection is completed with terms with L1 regularization (which can also be optimized with L2 penalty terms), and random forest mean impurity reduction method/mean accuracy reduction method.\n",
    "\n",
    "Advantages: The feature search is carried out around the learning algorithm, and any learning bias of the learning algorithm can be considered. The number of times to train the model is smaller than the Wrapper method, which saves time.\n",
    "\n",
    "Disadvantages: Slow operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection Method 1: Remove features with low variance.\n",
    "\n",
    "This method is generally used as a pre-processing work before feature selection, that is, the features with little value change are removed first, and then other feature selection methods are used to select features.\n",
    "\n",
    "By examining the variance value of the sample under a certain feature, it can be considered that given a threshold value, which features smaller than a certain threshold value can be discarded.\n",
    "\n",
    "#### 1. Implementation principle\n",
    "- Discrete variable:\n",
    "If the eigenvalues of a feature are only 0 and 1, and the value of the feature is 1 in 95% of all the input samples, then the feature is considered insignificant.\n",
    "If 100% of them are 1's, then this feature is meaningless.\n",
    "- Continuous variable:\n",
    "We need to discretize the continuous variables before we can use them.\n",
    "\n",
    "Moreover, in practice, there are generally not more than 95% of the characteristics of a certain value, so this method is simple but not very useful. It can be used as a pre-processing of feature selection, first removing those features with small value changes, and then selecting appropriate features from the feature selection methods mentioned next for further feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection method two: single variable feature selection\n",
    "\n",
    "Univariate feature selection method independently measures the relationship between each feature and the response variable. Univariate feature selection can test each feature, measure the relationship between the feature and the response variable, and discard the bad features according to the score. This method is simple, easy to run, easy to understand, usually for understanding data has a good effect (but not necessarily effective for feature optimization, improve generalization ability); There are many improved versions and variations of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Pearson Correlation\n",
    "Pearson's correlation coefficient is the simplest way to understand the relationship between characteristics and response variables. It measures the linear correlation between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the covariance of X_I and X_j is divided by the standard deviation of X_I and the standard deviation of X_J, which can be regarded as a special covariance after eliminating the dimensional influence of the two variables.\n",
    "Covariance measures the deviation degree of each dimension from its mean. If the value of covariance is positive, it indicates that they are positively correlated; otherwise, they are negatively correlated.\n",
    "The value range of the results is [-1, 1], -1 represents a complete negative correlation, +1 represents a complete positive correlation, 0 represents no linear correlation, and absolute value represents the strength of correlation.\n",
    "Standard deviation, also known as mean square deviation, is the arithmetic square root of variance and can reflect the degree of dispersion of a data set.\n",
    "2) It is mainly used for the screening of continuous features, not for the screening of discrete features.\n",
    "3) Advantages and disadvantages\n",
    "Advantages:\n",
    "Correlation coefficient calculation is fast and easy to calculate, and is often performed immediately after the data is obtained (after cleaning and feature extraction). Pearson correlation coefficient can represent rich relationship, coincidence represents positive and negative relationship, absolute value can represent strength.\n",
    "Disadvantages:\n",
    "As a feature ordering mechanism, the correlation coefficient is only sensitive to linear relations. If the relations are nonlinear, even if the two variables have a one-to-one correspondence, the correlation coefficient coefficient may be close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower noise (0.7261901361835682, 1.365052942629593e-164)\n",
      "Higher noise (0.09357375287756167, 0.003057851441699437)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "np.random.seed(2019)\n",
    "size=1000\n",
    "x = np.random.normal(0, 1, size)\n",
    "# 计算两变量间的相关系数\n",
    "print(\"Lower noise {}\".format(pearsonr(x, x + np.random.normal(0, 1, size))))\n",
    "print(\"Higher noise {}\".format(pearsonr(x, x + np.random.normal(0, 10, size))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Mutual Information and Maximal Information Coefficient\n",
    "\n",
    "If the variables are not independent, then we can determine whether they are \"close\" to being independent of each other by examining the Kullback-Leibler divergence between the joint probability distribution and the product of the marginal probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Mutual information method\n",
    "\n",
    "Entropy H (Y) and the conditional entropy H (Y | X) is called the mutual information of the difference between, mutual information and the relationship between the conditional entropy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I(x,y) = H(x) - H(x|y) = H(y) - H(y|x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, this is the feature selection rule of ID3 decision tree.\n",
    "\n",
    "Mutual information method is also used to evaluate the correlation between qualitative independent variables and qualitative dependent variables, but it is not convenient to be directly used in feature selection:\n",
    "\n",
    "It does not belong to the measurement method, and there is no way to normalize, so the results on different data cannot be compared.\n",
    "It can only be used for discrete feature selection. Continuous feature selection can only be carried out with mutual information before discretization, and the results of mutual information are very sensitive to the discretization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Maximum information coefficient method\n",
    "\n",
    "Since mutual information method is not convenient for feature selection, maximum information coefficient is introduced. The maximal information data first looks for an optimal discrete method, and then converts the mutual information value into a measurement method with the value interval of [0,1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Distance correlation\n",
    "The distance correlation coefficient is designed to overcome the weakness of Pearson correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's correlation coefficient is 0, and we cannot conclude that these two variables are independent (possibly non-linear correlation).\n",
    "\n",
    "For example, the Pearson correlation between x and x^2 is 0, but the two variables are not independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "\n",
    "# from numbapro import jit, float32\n",
    "\n",
    "def distcorr(X, Y):\n",
    "    \"\"\" Compute the distance correlation function\n",
    "\n",
    "    >>> a = [1,2,3,4,5]\n",
    "    >>> b = np.array([1,2,9,4,4])\n",
    "    >>> distcorr(a, b)\n",
    "    0.762676242417\n",
    "    \"\"\"\n",
    "    X = np.atleast_1d(X)\n",
    "    Y = np.atleast_1d(Y)\n",
    "    if np.prod(X.shape) == len(X):\n",
    "        X = X[:, None]\n",
    "    if np.prod(Y.shape) == len(Y):\n",
    "        Y = Y[:, None]\n",
    "    X = np.atleast_2d(X)\n",
    "    Y = np.atleast_2d(Y)\n",
    "    n = X.shape[0]\n",
    "    if Y.shape[0] != X.shape[0]:\n",
    "        raise ValueError('Number of samples must match')\n",
    "    a = squareform(pdist(X))\n",
    "    b = squareform(pdist(Y))\n",
    "    A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()\n",
    "    B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()\n",
    "\n",
    "    dcov2_xy = (A * B).sum()/float(n * n)\n",
    "    dcov2_xx = (A * A).sum()/float(n * n)\n",
    "    dcov2_yy = (B * B).sum()/float(n * n)\n",
    "    dcor = np.sqrt(dcov2_xy)/np.sqrt(np.sqrt(dcov2_xx) * np.sqrt(dcov2_yy))\n",
    "    return dcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model Based Ranking\n",
    "\n",
    "The idea is to directly use the machine learning algorithm you want to use to build a prediction model for each individual feature and response variable. If the relationship between features and response variables is nonlinear, there are many alternatives, such as tree-based approaches (decision trees, random forests), or extended linear models. Tree-based approaches are one of the simplest because they can model nonlinear relationships well and do not require much tweaking. But the main thing to avoid is overfitting, so the tree depth should be relatively small, and cross validation should be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Chi-square test\n",
    "The Chi-square test, proposed by Karl Pearson, is a widely used hypothesis testing method for counting data. The chi-square value describes the independence of the two events or the degree to which the actual observed value deviates from the expected value. The larger the chi-square value is, the larger the deviation between the actual observed value of the table name and the expected value is, which also indicates that the independence of the two events is weaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Principle introduction\n",
    "\n",
    "The CHI value (CHI square value) is used to measure the difference between the actual value and the theoretical value. In order to avoid the deviation between different observed values and different expectations, it is divided by T, so it is divided by E to eliminate this disadvantage.\n",
    "\n",
    "The absolute size of the deviation between the actual value and the theoretical value (the difference is magnified by the presence of squares)\n",
    "The relative size of the difference and the theoretical value\n",
    "\n",
    "2) Implementation process\n",
    "\n",
    "The higher the CHI value is, the less likely the two variables are to be independent. In other words, the higher the CHI value is, the higher the correlation degree of the two variables is.\n",
    "A. For characteristic variables x1,x2... ,xn, and the classification variable y. Just compute CHI(x1,y), CHI(x2,y),... , CHI(xn,y), and sort the features according to the value of CHI from large to small.\n",
    "B. Select an appropriate threshold. Features larger than the threshold are retained, and features smaller than the threshold are deleted. A subset of features screened out in this way is the feature of input model training.\n",
    "\n",
    "3) It can only be used for the screening of discrete features in classification problems, but not for the screening of continuous features in classification problems, nor for the feature screening of regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realistic approach:\n",
    "\n",
    "Sklearn. Feature_selection. SelectKBest:\n",
    "Returns k best features\n",
    "\n",
    "Sklearn. Feature_selection. SelectPercentile:\n",
    "Returns the top r% of the best performing characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Summary\n",
    "\n",
    "- The method of removing the features with small value changes is generally used as a pre-processing work before feature selection, that is, removing the features with small value changes first, and then using other feature selection methods to select the features. If the machine has sufficient resources and you want to retain as much information as possible, you can set the threshold high or filter only discrete features with only one value.\n",
    "\n",
    "- Univariate feature selection can be used to understand data, data structure, characteristics, or to exclude irrelevant features, but it cannot find redundant features.\n",
    "\n",
    "- The feature method with small value change and the single variable feature selection method belong to the filtering class feature selection method, but the learning algorithm cannot transmit the feature requirement to the feature search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
