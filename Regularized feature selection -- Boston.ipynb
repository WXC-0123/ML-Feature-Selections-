{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized feature selection -- Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Main idea\n",
    "When all features are at the same scale, the most important feature should have the highest coefficient in the model, while the feature unrelated to the output variable should have coefficient values close to zero. Even using simple linear regression models, this approach works well when the data is not very noisy (or there is a large number of data compared to the number of features) and the features are (relatively) independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regularization model\n",
    "Regularization is to add additional constraints or penalties to the existing model (loss function) to prevent overfitting and improve generalization ability. Loss function E (X, Y) has E (X, Y) + alpha | | w | |, w is a coefficient model of vector (also called parameters parameter in some places, the coefficients), | |, | | is commonly L1 and L2 norm, alpha is an adjustable parameters, controls the strength of the regularization. When used on linear models, L1 and L2 regularization are also known as Lasso and Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) L1 regularization /Lasso Regression\n",
    "L1 regularization adds the L1 norm of the coefficient W to the loss function as a penalty term. Since the regular term is non-zero, this forces the coefficients corresponding to weak features to be zero. Therefore, L1 regularization tends to make the learned models sparse (the coefficient W is often 0), which makes L1 regularization a good feature selection method.\n",
    "\n",
    "Lasso was able to pick out some good features while letting the coefficients of others go to zero. It is useful when you need to reduce the number of features, but not very useful for data understanding.\n",
    "\n",
    "2) L2 Regularization /Ridge Regression\n",
    "L2 regularization adds the L2 norm of the coefficient vector to the loss function.\n",
    "\n",
    "Since the coefficients in THE penalty term L2 are quadratic, there are many differences between L2 and L1. The most obvious one is that L2 regularization will average the values of coefficients.\n",
    "For correlation features, this means that they can obtain more similar corresponding coefficients.\n",
    "The Ridge distributes the regression coefficients evenly across the associated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 regularization is a stable model for feature selection, unlike L1 regularization, where coefficients fluctuate with subtle data changes. Therefore, L2 regularization and L1 regularization provide different values, and L2 regularization is more useful for feature understanding: the coefficient corresponding to a feature with strong capability is non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature has its corresponding weight coefficient coef. The positive and negative values of the feature's weight coefficient represent whether the feature is positively or negatively correlated with the target value, and the absolute value of the feature's weight coefficient represents importance.\n",
    "\n",
    "The fit() method for the LinearRegression in SKLearn is to solve θ through the training set. The two properties of the LinearRegression intercept and COef correspond to θ0 and θ1-θn, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureNameSort: ['NOX' 'DIS' 'PTRATIO' 'LSTAT' 'CRIM' 'INDUS' 'AGE' 'TAX' 'B' 'ZN' 'RAD'\n",
      " 'CHAS' 'RM']\n",
      "featureCoefSore: [-1.23981083e+01 -1.21096549e+00 -8.38180086e-01 -3.50107918e-01\n",
      " -1.06715912e-01 -4.38830943e-02 -2.36790549e-02 -1.37774382e-02\n",
      "  7.85316354e-03  3.53133180e-02  2.51301879e-01  4.52209315e-01\n",
      "  3.75945346e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#获取boston数据\n",
    "boston=datasets.load_boston()\n",
    "x=boston.data\n",
    "y=boston.target\n",
    "\n",
    "b = datasets.load_boston() \n",
    "\n",
    "bos = pd.DataFrame(b.data)\n",
    "bos.columns = b.feature_names\n",
    "X = bos[bos.columns]\n",
    "bos[\"PRICE\"] = b.target\n",
    "y = bos[\"PRICE\"]\n",
    "\n",
    "#过滤掉异常值\n",
    "X=X[y<50]\n",
    "y=y[y<50]\n",
    "reg=LinearRegression()\n",
    "reg.fit(X,y)\n",
    "#求排序后的coef\n",
    "coefSort=reg.coef_.argsort()\n",
    "#featureNameSort: 按对标记值的影响，从小到大的各特征值名称\n",
    "#featureCoefSore：按对标记值的影响，从小到大的coef_\n",
    "featureNameSort=boston.feature_names[coefSort]\n",
    "featureCoefSore=reg.coef_[coefSort]\n",
    "print(\"featureNameSort:\", featureNameSort)\n",
    "print(\"featureCoefSore:\", featureCoefSore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results analysis:\n",
    "\n",
    "- The largest eigenvalue of the positive correlation effect coefficient is \"RM\" : the average number of rooms, and the coefficient value is 3.75.\n",
    "- The largest characteristic value of negative correlation coefficient is \"NOX\" : nitric oxide concentration, and the coefficient value is -1.24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.24227912,  0.081819  , -0.        ,  0.53987192, -0.69891258,\n",
       "        2.99322993, -0.        , -1.08091325,  0.        , -0.        ,\n",
       "       -1.75561249,  0.62831526, -3.70463287])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "\n",
    "def pretty_print_linear(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name) for coef, name in lst)\n",
    "\n",
    "boston = load_boston()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(boston[\"data\"])\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "\n",
    "lasso = Lasso(alpha=.3)\n",
    "lasso.fit(X, Y)\n",
    "lasso.coef_\n",
    "# print(\"Lasso model: {}\".format(pretty_print_linear(lasso.coef_, names, sort = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 0\n",
      "Linear model: 0.728 * X0 + 2.309 * X1 + -0.082 * X2\n",
      "Ridge model: 0.938 * X0 + 1.059 * X1 + 0.877 * X2\n",
      "Random seed 1\n",
      "Linear model: 1.152 * X0 + 2.366 * X1 + -0.599 * X2\n",
      "Ridge model: 0.984 * X0 + 1.068 * X1 + 0.759 * X2\n",
      "Random seed 2\n",
      "Linear model: 0.697 * X0 + 0.322 * X1 + 2.086 * X2\n",
      "Ridge model: 0.972 * X0 + 0.943 * X1 + 1.085 * X2\n",
      "Random seed 3\n",
      "Linear model: 0.287 * X0 + 1.254 * X1 + 1.491 * X2\n",
      "Ridge model: 0.919 * X0 + 1.005 * X1 + 1.033 * X2\n",
      "Random seed 4\n",
      "Linear model: 0.187 * X0 + 0.772 * X1 + 2.189 * X2\n",
      "Ridge model: 0.964 * X0 + 0.982 * X1 + 1.098 * X2\n",
      "Random seed 5\n",
      "Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2\n",
      "Ridge model: 0.758 * X0 + 1.011 * X1 + 1.139 * X2\n",
      "Random seed 6\n",
      "Linear model: 1.199 * X0 + -0.031 * X1 + 1.915 * X2\n",
      "Ridge model: 1.016 * X0 + 0.89 * X1 + 1.091 * X2\n",
      "Random seed 7\n",
      "Linear model: 1.474 * X0 + 1.762 * X1 + -0.151 * X2\n",
      "Ridge model: 1.018 * X0 + 1.039 * X1 + 0.901 * X2\n",
      "Random seed 8\n",
      "Linear model: 0.084 * X0 + 1.88 * X1 + 1.107 * X2\n",
      "Ridge model: 0.907 * X0 + 1.071 * X1 + 1.008 * X2\n",
      "Random seed 9\n",
      "Linear model: 0.714 * X0 + 0.776 * X1 + 1.364 * X2\n",
      "Ridge model: 0.896 * X0 + 0.903 * X1 + 0.98 * X2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "size = 100\n",
    "\n",
    "#We run the method 10 times with different random seeds\n",
    "for i in range(10):\n",
    "    print(\"Random seed {}\".format(i))\n",
    "    np.random.seed(seed=i)\n",
    "    X_seed = np.random.normal(0, 1, size)\n",
    "    X1 = X_seed + np.random.normal(0, .1, size)\n",
    "    X2 = X_seed + np.random.normal(0, .1, size)\n",
    "    X3 = X_seed + np.random.normal(0, .1, size)\n",
    "    Y = X1 + X2 + X3 + np.random.normal(0, 1, size)\n",
    "    X = np.array([X1, X2, X3]).T\n",
    "\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,Y)\n",
    "    print(\"Linear model: {}\".format(pretty_print_linear(lr.coef_)))\n",
    "\n",
    "    ridge = Ridge(alpha=10)\n",
    "    ridge.fit(X,Y)\n",
    "    print(\"Ridge model: {}\".format(pretty_print_linear(ridge.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the examples, the coefficients of linear regression vary widely, depending on the data generated. However, for the L2 regularization model, the coefficients are very stable and closely reflect the way the data is generated (all coefficients are close to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
