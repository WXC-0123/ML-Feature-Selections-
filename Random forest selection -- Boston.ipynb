{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest selection -- Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Main idea\n",
    "Random forest has the advantages of high accuracy, good robustness and easy to use, making it one of the most popular machine learning algorithms at present. Random forest provides two methods of feature selection: mean decrease impurity and mean decrease accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mean decrease in impurity\n",
    "\n",
    "Principle introduction\n",
    "- The random forest is composed of multiple CART decision trees, in which each node is a condition about a certain feature, in order to divide the dataset into two parts according to different response variables.\n",
    "- CART uses impurity to determine nodes (optimal conditions). Gini impurity is usually used for classification problems, and variance or least square fitting is usually used for regression problems.\n",
    "- When training the decision tree, you can calculate how much each feature reduces the purity of the tree. For a decision tree forest, it is possible to calculate how much impurity each feature reduces on average, and use the average reduction of impurity as the criterion for feature selection.\n",
    "- The ranking result of random forest based on impurity is very bright, and the score of the features after the features with the highest score drops sharply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "[(0.432, 'RM'), (0.3777, 'LSTAT'), (0.0599, 'DIS'), (0.0345, 'CRIM'), (0.0273, 'NOX'), (0.0162, 'PTRATIO'), (0.0153, 'TAX'), (0.0127, 'AGE'), (0.0114, 'B'), (0.0071, 'INDUS'), (0.0038, 'RAD'), (0.0011, 'ZN'), (0.0009, 'CHAS')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "#Load boston housing dataset as an example\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "rf = RandomForestRegressor()\n",
    "# 训练随机森林模型，并通过feature_importances_属性获取每个特征的重要性分数。rf = RandomForestRegressor()\n",
    "rf.fit(X, Y)\n",
    "print(\"Features sorted by their score:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names),\n",
    "             reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mean decrease in accuracy\n",
    "\n",
    "Principle introduction\n",
    "- Feature selection is carried out by directly measuring the effect of each feature on model accuracy.\n",
    "- The main idea is to disrupt the order of the eigenvalues of each feature and measure the effect of the order change on the accuracy of the model.\n",
    "- For unimportant variables, shuffling order does not have much effect on the accuracy of the model.\n",
    "- For important variables, disordering reduces the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a very popular feature selection method and it is easy to use. But it has two main problems:\n",
    "- Important features may score low (correlation feature questions)\n",
    "- This method is more favorable for features with more categories of feature variables (bias problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, feature screening is to understand the data or better train the model, we should choose suitable methods according to our own goals. Feature filtering for better/easier training of models should be avoided if computational resources are sufficient, as feature filtering can easily lose useful information. In order to reduce the influence of invalid features and avoid over-fitting, random forest and XGBoost integration models can be selected to avoid over-fitting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
