{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# three methods for feature selection\n",
    "\n",
    "# 1. Background\n",
    "The Titanic dataset is ideal for beginners to data science and machine learning.\n",
    "\n",
    "The data set includes personal information and survival status of some of the crew members of the Titanic shipwreck in 1912. These historical data have been divided into training sets and test sets, which can be used to develop suitable models and predict the survival status of test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic=pd.read_csv('train.csv')\n",
    "titanic.head(3)\n",
    "print(titanic.describe())\n",
    "print(titanic.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas reads the CSV file into the DataFrame format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The describe function\n",
    "\n",
    "View the output using the describe function to summarize the central trend, dispersion, and shape of the distribution of the data set, excluding NaN values.\n",
    "DataFrame. The describe (percentiles = None, include = None, exclude = None).\n",
    "You can quickly figure out some arithmetic operations. The others are easy to understand, STD stands for standard deviation.\n",
    "Include includes all, [Np. number], and [NP. object]. The describe attribute enables descriptive statistics of numerical variables (include=[' number ']) and discrete variables (include=[' object '])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PassengerId    Survived      Pclass                Name   Sex  \\\n",
      "count    891.000000  891.000000  891.000000                 891   891   \n",
      "unique          NaN         NaN         NaN                 891     2   \n",
      "top             NaN         NaN         NaN  Richard, Mr. Emile  male   \n",
      "freq            NaN         NaN         NaN                   1   577   \n",
      "mean     446.000000    0.383838    2.308642                 NaN   NaN   \n",
      "std      257.353842    0.486592    0.836071                 NaN   NaN   \n",
      "min        1.000000    0.000000    1.000000                 NaN   NaN   \n",
      "25%      223.500000    0.000000    2.000000                 NaN   NaN   \n",
      "50%      446.000000    0.000000    3.000000                 NaN   NaN   \n",
      "75%      668.500000    1.000000    3.000000                 NaN   NaN   \n",
      "max      891.000000    1.000000    3.000000                 NaN   NaN   \n",
      "\n",
      "               Age       SibSp       Parch    Ticket        Fare    Cabin  \\\n",
      "count   714.000000  891.000000  891.000000       891  891.000000      204   \n",
      "unique         NaN         NaN         NaN       681         NaN      147   \n",
      "top            NaN         NaN         NaN  CA. 2343         NaN  B96 B98   \n",
      "freq           NaN         NaN         NaN         7         NaN        4   \n",
      "mean     29.699118    0.523008    0.381594       NaN   32.204208      NaN   \n",
      "std      14.526497    1.102743    0.806057       NaN   49.693429      NaN   \n",
      "min       0.420000    0.000000    0.000000       NaN    0.000000      NaN   \n",
      "25%      20.125000    0.000000    0.000000       NaN    7.910400      NaN   \n",
      "50%      28.000000    0.000000    0.000000       NaN   14.454200      NaN   \n",
      "75%      38.000000    1.000000    0.000000       NaN   31.000000      NaN   \n",
      "max      80.000000    8.000000    6.000000       NaN  512.329200      NaN   \n",
      "\n",
      "       Embarked  \n",
      "count       889  \n",
      "unique        3  \n",
      "top           S  \n",
      "freq        644  \n",
      "mean        NaN  \n",
      "std         NaN  \n",
      "min         NaN  \n",
      "25%         NaN  \n",
      "50%         NaN  \n",
      "75%         NaN  \n",
      "max         NaN  \n"
     ]
    }
   ],
   "source": [
    "print(titanic.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>681</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Richard, Mr. Emile</td>\n",
       "      <td>male</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name   Sex    Ticket    Cabin Embarked\n",
       "count                  891   891       891      204      889\n",
       "unique                 891     2       681      147        3\n",
       "top     Richard, Mr. Emile  male  CA. 2343  B96 B98        S\n",
       "freq                     1   577         7        4      644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "titanic.describe(include=[np.object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The info function\n",
    "\n",
    "The datafame.info () function is used to get a brief summary of the dataframe. It comes in handy for exploratory analysis of data. To quickly navigate through the data set, we use the datafame.info () function.\n",
    "\n",
    "1. Usage:\n",
    "\n",
    "DataFrame.info(verbose=None, buf=None, max_cols=None, memory_usage=None, null_counts=None)\n",
    "\n",
    "2. Parameter analysis:\n",
    "\n",
    "- verbose: Indicates whether to print a complete summary. Nothing will be displayed on the screen.\n",
    "\n",
    "- max_info_columns Settings. True or False overrides the display. Max_info_columns Settings.\n",
    "- buf: writable buffer. The default value is sys.stdout\n",
    "- max_cols: Determines whether to print a full summary or a short summary. Nothing will be displayed on the screen. Max_info_columns Settings.\n",
    "- memory_usage: Specifies whether the total memory usage of DataFrame elements (including indexes) should be displayed. Nothing will be displayed on the screen. Memory_usage Settings. True or False overrides the display. Memory_usage Settings. \"Deep\" has the same value as True and is introspective. Memory usage is shown in human-readable units, represented in base 2.\n",
    "- NULl_COUNTS: Specifies whether to display non-null counts. If None, only whether the frame is smaller than max_info_ROWS and max_info_columns are displayed. If True, the count is always displayed. If False, the count is never displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Columns: 12 entries, PassengerId to Embarked\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the dataset\n",
    "\n",
    "The number of individual characteristic values output by the info() function, PassengerId: 891, survives: 891, Pclass: 891, Name: 891, Sex: 891, Age: 714, SibSp: 891, Parch: 891, Ticket: Embarked: 891, Fare: 891, Cabin: 204, Embarked: 889, there are 891 passengers in total, but the values of Age, Cabin, and Embarked are not sufficient, and the missing values need to be filled. Note that Name, Sex, Ticket, Cabin, and pursuit are all object data types that cannot be processed by machine learning. Therefore, convert these data types to those that can be processed by machine learning. Every data analysis requires data preprocessing. Data preprocessing is a necessary step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge of data preprocessing\n",
    "\n",
    "\n",
    "Data is the raw material of machine learning, and we need to process the data before putting it into the machine learning model. Data preprocessing is very important in every link of data analysis! Data preprocessing is the most troublesome but challenging part of machine learning. In real business processes, data is usually dirty. Therefore, data preprocessing is also called data cleaning. Data may have the following problems before preprocessing:\n",
    "\n",
    "1. Incomplete data is a condition where the attribute value is empty. For example, Age= \".\n",
    "2. Data noise is the condition where data values are abnormal. Such as Ticket = \"100\"\n",
    "3. Inconsistent data refers to data that is Inconsistent. Age = \"42\" vs. Birthday = \"01/09/1985\"\n",
    "Data redundancy refers to a situation where the number of data or attributes exceeds the data analysis requirements. For example, the number of Survived is 1000.\n",
    "5. Data set Imbalance is a situation in which various types of data amount differ greatly. (Normalization is usually used here to get their data levels closer together.)\n",
    "6. Outliers are data that is far from the rest of the data set. For example, there is an old woman in a group of children, who are quite different in age.\n",
    "7. Duplicate data refers to data that appears multiple times in a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean data\n",
    "In the data cleaning phase, we deal with missing data, outliers and duplicate data mentioned in part 1.\n",
    "\n",
    "Missing data\n",
    "In this Titanic dataset, the missing data is processed.\n",
    "There are mainly the following categories:\n",
    "\n",
    "- 1.Missing completely at random: The probability of Missing is random. For example, the data of the counter in the store is empty in a certain period of time due to power failure and network interruption.\n",
    "- 2.Missing conditionally at random: Whether the data are Missing depends on another attribute, such as the reluctance of some girls to fill in their weight.\n",
    "- 3.It's true: The missing data is related to their own values, such as people with high incomes who may Not be willing to report their income.\n",
    "\n",
    "There are several ways to deal with it:\n",
    "- A. Delete data. If the proportion of missing data records is small, delete these records directly.\n",
    "- B. Manually fill in the data, or re-collect the data, or supplement the data according to the domain knowledge.\n",
    "- C. Automatic filling, simply mean filling, or add a probability distribution to make it seem more realistic. It can also be calculated by a formula based on the actual situation, such as missing store count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outliers\n",
    "An outlier is data that is far from the rest of the data set, which may be generated by random factors or by different mechanisms. How to deal with outliers depends on the cause and purpose of application. If it is generated by random factors, we ignore or eliminate outliers. If generated by a different mechanism, outliers are precious and the focus of the application. Which an application for abnormal behavior detection, such as in the bank's credit card fraud recognition, based on a lot of the credit card user information and consumer behavior to the quantitative modeling and clustering, found clustering points away from a large number of sample is very suspicious, because their characteristic and general credit card users, their consumption behavior and general credit card consumption behavior also. There are also shopping websites to detect malicious brushing and other scenarios also focus on the analysis of outliers.\n",
    "Whether to propose outliers or focus on research and application, we need to detect outliers first. Several methods are provided in SkLearn (a Python machine learning package), such as OneClassSVM, Isolation Forest, Local Outlier Factor (LOF).\n",
    "\n",
    "### Duplicate data\n",
    "The processing of repeated data is as follows: If highly suspected samples are next to each other, a sliding window can be used for comparison. In order to make similar records adjacent, a hash key can be generated for each record and sorted according to the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data conversion\n",
    "In the data conversion stage, we carry on sampling processing, type conversion and normalization to the data.\n",
    "In Titanic data set, data type conversion is mainly carried out\n",
    "\n",
    "### Sampling process\n",
    "Sampling is the process of taking sample points from a particular probability distribution.\n",
    "\n",
    "Sampling has very important applications in machine learning: reducing complex distributions to discrete sample points; Resampling can be used to adjust the sample set to better adjust and adapt to later model learning. Used for stochastic simulation to approximate solution or inference of complex models. An important role of sampling is to deal with unbalanced data sets.\n",
    "\n",
    "The simplest way to deal with an unbalanced sample set is random sampling. Sampling is generally divided into over-sampling and under-sampling. Random over-sampling is to randomly and repeatedly extract samples from a minority class sample set S_min, while random under-sampling is to randomly select a few samples from a majority class sample set S_max. The two methods also have problems. For example, random oversampling will enlarge the data scale and easily cause overfitting. Random undersampling may lose some useful information, resulting in underfitting. In order to solve the problem of appeal, it usually takes some method to generate new samples instead of simply copying samples. SMOTE (Synthetic Minority Oversampling Technique) algorithm, Borderline-SMOTE, ADASYN and other algorithms. For Undersampling, Informed Undersampling can be adopted to solve the problem of data loss.\n",
    "- PS: When the total amount of data is insufficient, in addition to simplifying the model, we can use the method of random over-sampling to over-sample each class. Specific to the image task, can also be directly transformed in the image space, such as by applying a certain range of transformation to the image (rotation, translation, scaling, cutting, filling, flipping, adding noise, color transformation, change brightness, clarity, contrast, etc.), to get the expanded data set. In addition, transfer learning is a good way to model on small data sets.\n",
    "- PS: The overall accuracy is not applicable to unbalanced data sets, so a new measurement mode should be introduced, such as G-mean, which will look at the accuracy on the positive class and then the accuracy on the negative class, and then multiply the two to take the square root. Another common metric is f-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversion\n",
    "Data types can be simply divided into numeric and non-numeric types. Numerical type has continuous type and discrete type. Non-numeric type has category type and non-category type. In the category type feature, if the category has sorting problem, it is fixed type; if there is no sorting problem, it is fixed type; non-category type is string type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data description\n",
    "In the data description stage, we can calculate statistics and visualize the data as needed.\n",
    "\n",
    "The general description of data includes mean, median, mode, and variance.\n",
    "Mean; Median is the median, and the value in the middle of the sorted data is taken to avoid the influence of extreme outliers on objective evaluation. Mode is the element that appears most frequently, but is actually used less often; Variance is the deviation from the mean of a data set measured by variance.\n",
    "\n",
    "The correlation between data can be measured using Pearson correlation coefficient and Pearson Chi-square. The former applies to the case with metric data, while the latter applies to the case with categorical statistics.\n",
    "Data visualization one-dimensional data pie chart, bar chart; Two-dimensional data scatter diagram; 3d data is presented by 3D coordinates; High-dimensional data will need to be converted or mapped, such as by matlab Box Plots, and will also be rendered in parallel coordinates. There are many tools available, such as MATLAB and Geph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic data preprocessing\n",
    "\n",
    "The data cleaning is mainly aimed at the problem of data loss, and the characteristics Age, Sex, and Embarked are pursued.\n",
    "\n",
    "1. Fill in the missing value of Age discrete data\n",
    "Fillna () function is generally used for missing value filling, which can be divided into two cases:\n",
    "- 1. If it is a numeric type, replace it with the average or median\n",
    "- 2. If the data is categorized, replace it with the most common category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Age']=titanic['Age'].fillna(titanic['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.loc[titanic['Sex']=='male','Sex']=0\n",
    "titanic.loc[titanic['Sex']=='female','Sex']=1\n",
    "titanic['Sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of unique() here,\n",
    "Reprinted from unique() usage\n",
    "\n",
    "Unique (a) removes duplicate elements from a one-dimensional array or list and returns a new tuple or list with no duplicate elements in order of size.\n",
    "c,s=np.unique(b,return_index=True)\n",
    "Return_index =True returns the position of the new list element in the old list and stores it as a list in S.\n",
    "a, s,p = np.unique(A, return_index=True, return_inverse=True)\n",
    "Return_inverse =True returns the position of the old list elements in the new list and is stored in p as a list\n",
    "And loc[] :\n",
    "Titanic. Loc [0] represents the sample in line 0\n",
    "Loc [0, 'PassengerId'] indicates behavior 0, as the value of PassengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Embarked'].describe()\n",
    "titanic['Embarked'].unique()\n",
    "titanic['Embarked'] = titanic['Embarked'].fillna('S')\n",
    "titanic.loc[titanic['Embarked'] == 'S', 'Embarked'] = 0\n",
    "titanic.loc[titanic['Embarked'] == 'C', 'Embarked'] = 1\n",
    "titanic.loc[titanic['Embarked'] == 'Q', 'Embarked'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Feature selection is to eliminate irrelevant or redundant features, reduce the number of effective features, reduce the time of model training, and improve the accuracy of the model. Feature extraction achieves dimensionality reduction through feature transformation, while feature selection relies on statistical methods or feature selection (sorting) of machine learning model to achieve dimensionality reduction. Feature selection is a process of repeated iteration. Sometimes, we may think that feature selection is good, but model training is not so good in practice. Therefore, every feature selection is verified by model, with the ultimate goal of obtaining data that can train a good model and improving the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Use statistical methods to measure the relationship between a single feature and response variable (Lable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ① Pearson Correlation Coefficient\n",
    "To measure the linear relationship between variables, the result is [-1,1], -1 means completely negative correlation, +1 means positive correlation, 0 means non-linear correlation, but does not mean that there is no other relationship. An obvious defect of Pearson's coefficient is that it only measures the linear relationship. If the relationship is nonlinear, even if there is a one-to-one corresponding relationship among all variables, Pearson's relationship will be close to 0. Pearson's coefficient is given as the sample covariance divided by the standard deviation of X multiplied by the standard deviation of Y.\n",
    "\n",
    "- ② Maximum Information Coefficient (MIC)\n",
    "MIC, the information coefficient, can not only measure the linear relationship between variables like Pearson's coefficient, but also measure the nonlinear relationship between variables. MIC can measure the nonlinear relationship between variables, but when the relationship between variables is close to linear correlation, Pearson correlation coefficient is still irreplaceable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly used feature selection method is to directly measure the effect of each feature on model accuracy. The main idea is to disrupt the order of the eigenvalues of each feature and measure the effect of the order change on the accuracy of the model. Obviously, for unimportant variables, shuffling order does not have much effect on the accuracy of the model, but for important variables, shuffling order reduces the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember:\n",
    "\n",
    "- 1. There is bias in this method, which is more favorable for variables with more categories; \n",
    "- 2. there is a connection for multiple characteristics, in which any one can be as an indicator (features), and once after a feature is selected, other features will be a sharp drop in the important degree, because no purity has been selected the features down, other characteristic is hard to reduce so much not purity, as a result, Only the feature selected first has high importance, and the other related features tend to have low importance. This can lead to misunderstandings when interpreting the data, leading to the mistaken belief that the features selected first are important and the rest are not, when in fact they are very close to the response variable (much like Lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Feature selection based on machine learning model\n",
    "\n",
    "- ① Linear model and regularization\n",
    "When all the features and response variables are linear, and the features are relatively independent. We can try to use the linear regression model for feature selection, because the more important the feature is, the larger the corresponding coefficient will be in the model, and the more irrelevant the feature is to the output variable, the closer the corresponding coefficient will be to 0.\n",
    "\n",
    "- ② Random forest model\n",
    "Random forest is composed of multiple decision trees, and each node in the decision tree is the condition of a certain feature. The optimal feature for dividing data sets can be determined by using the impurity. For classification problems, Gini impurity or information gain is usually adopted, and for regression problems, variance or least square fitting is usually adopted. When training the decision tree, you can calculate how much each feature reduces the purity of the tree. For a decision tree forest, it is possible to calculate how much impurity is reduced on average by each feature and take the average reduction of impurity as the value of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     517\n",
      "2     182\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "8       2\n",
      "7       2\n",
      "9       2\n",
      "16      1\n",
      "10      1\n",
      "11      1\n",
      "12      1\n",
      "13      1\n",
      "14      1\n",
      "15      1\n",
      "17      1\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#提炼新特征\n",
    "titanic['FamilySize']=titanic['SibSp']+titanic['Parch']#家庭规模：兄弟姐妹数量加上父母数量\n",
    "titanic['NameLength']=titanic['Name'].apply(lambda x:len(x))\n",
    "import re\n",
    "import pandas as pd\n",
    "def get_title(name):\n",
    "    title_search=re.search(' ([A-Za-z]+)\\.', name)\n",
    "    \n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return ''\n",
    "titles=titanic['Name'].apply(get_title)\n",
    "title_mapping={\"Mr\":1,\"Miss\":2,\"Mrs\":3,\"Master\":4,\"Dr\":5,\"Rev\":6,\"Major\":7,\"Col\":8,\"Mlle\": 9,\n",
    "                 \"Mme\": 10, \"Don\": 11, \"Lady\": 12, \"Countess\": 13, \"Jonkheer\": 14, \"Sir\": 15, \"Capt\": 16, \"Ms\": 17}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles==k]=v\n",
    "print(pd.value_counts(titles))\n",
    "titanic['Title']=titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \n",
    "\"FamilySize\", \"NameLength\", \"Title\"]\n",
    "selector = SelectKBest(f_classif, k=5)# 方差分析，计算方差分析（ANOVA）的F值 (组间均方 / 组内均方)，选取前5个特征\n",
    "selector.fit(titanic[predictors], titanic['Survived'])\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression (the simplest regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5342312008978676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "#选择特征\n",
    "predictors=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\n",
    "#导入线性回归\n",
    "alg=LinearRegression()\n",
    "#将样本分成三份进行交叉验证\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "# kf=KFold(titanic.shape[0],n_splits=3,random_state=1)\n",
    "predictions=[]\n",
    "# for train_index,test_index in kf:\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    #用于训练的特征数据\n",
    "    train_predictors=titanic[predictors].iloc[train_index,:]\n",
    "    #特征数据的label（即是否获救）\n",
    "    train_target=titanic['Survived'].iloc[train_index]\n",
    "    #训练线性回归模型\n",
    "    alg.fit(train_predictors,train_target)   \n",
    "    test_predictions=alg.predict(titanic[predictors].iloc[test_index,:])\n",
    "    predictions.append(test_predictions)\n",
    "#线性回归得到的结果是在[0,1]\n",
    "import numpy as np\n",
    "# predictions[predictions > .5]=1\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <= .5] = 0\n",
    "accuracy=sum(predictions==titanic['Survived'])/len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Cross_val_score (model_name, X,y, CV =k)\n",
    "Parameters: 1. Model function name, such as LogisticRegression()2. Training set 3\n",
    "Function: verify the stability of a certain model on a certain training set, and output K prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79124579 0.79461279 0.8013468 ]\n",
      "0.7957351290684623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#初始化模型\n",
    "alg=LogisticRegression(random_state=1)\n",
    "scores=cross_val_score(alg,titanic[predictors],titanic['Survived'],cv=3)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests\n",
    "\n",
    "The randomness of the random forest is reflected in two points: 1. The sample is random and has been put back; 2. The selection of features is random, and not all attributes and features need to be used. Forest represents the generation of multiple decision trees\n",
    "\n",
    "- Function: RandomForestClassifier ()\n",
    "\n",
    "Parameter Description: random_state = 1 indicates that the code runs several times to get the same random value, if not set, the two execution of the random value is not the same; N_estimators =50 indicates that there are 50 decision trees; The conditions of tree splitting are as follows: min_samples_split =4 means that samples are constantly split. If there are only 4 samples on a node, the splitting will not continue. Min_samples_leaf =2 indicates that the minimum number of leaf nodes is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7957351290684623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "alg=RandomForestClassifier(random_state=1,n_estimators=10,min_samples_split=2,min_samples_leaf=1)\n",
    "kf = KFold(n_splits=3,  shuffle=True, random_state=1)\n",
    "acores=cross_val_score(alg,titanic[predictors],titanic['Survived'],cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid model\n",
    "\n",
    "Mixed models are commonly used in competitions: integrate multiple models, get results for each model, assign weight to each model, and get an average result.\n",
    "\n",
    "As for the mixed model, it is a common solution for time series prediction.\n",
    "\n",
    "The baseline model above found that logistic regression and random forest worked better, so I integrated both models. If a model is found to be better, the weight can be higher (random forest weight 2 in the case, logistic regression weight 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8338945005611672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuechunwang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "import numpy as np\n",
    "algorithms=[\n",
    "    [RandomForestClassifier(random_state=1,n_estimators=20,min_samples_split=4,min_samples_leaf=2),\n",
    "     ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'NameLength', 'Title']],\n",
    "    [LogisticRegression(random_state=1),\n",
    "     ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'NameLength', 'Title']]\n",
    "]\n",
    "kf = KFold(n_splits=11, shuffle=False)\n",
    "predictions=[]\n",
    "for train,test in kf.split(titanic):\n",
    "    train_target=titanic['Survived'].iloc[train]\n",
    "    full_test_prediction=[]\n",
    "    for alg,predictors in algorithms:\n",
    "        alg.fit(titanic[predictors].iloc[train,:],train_target)\n",
    "        test_prediction=alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_prediction.append(test_prediction)\n",
    "    #使用集成模型\n",
    "    test_predictions=(full_test_prediction[0]*2+full_test_prediction[1]*1)/3\n",
    "    test_predictions[test_predictions>.5]=1\n",
    "    test_predictions[test_predictions<=.5]=0\n",
    "    predictions.append(test_predictions)\n",
    "predictions=np.concatenate(predictions,axis=0)\n",
    "accuracy=sum(predictions==titanic['Survived'])/len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
